---
layout: post
title:  "Linked Data Step 3"
date:   2018-08-19 18:49:00
categories: semantic sparql
---

[Linked Data Step 1](/semantic/sparql/2017/06/09/lod-phase1/) talked about converting the DRI XML metadata to RDF. [Step 2](/semantic/sparql/2018/04/02/lod-phase2/) then described using OpenRefine to reconcile the metadata with other datasets; namely, DBpedia, LCSH (for subjects) and Linked Logainm (for places).

The final step is to import the reconciled RDF, exported from OpenRefine, together with the RDF output generated by the DRI application into a triple store to allow for querying. The full workflow then, comprised of each of the three steps, is as shown in the figure below.

[![Workflow]({{ site.url }}/assets/lod_worklow.png)]({{ site.url }}/assets/lod_workflow.png)

The RDF generated by the application from the object's XML metadata follows the guidelines produced by the [BBC's Research and Education Space (RES) Project][res-project]. To automate the process of importing the application's RDF output into the triple store we can use more components from that project. Anansi is a web crawler that 'includes specific support for Linked Data'. I can select the objects to crawl by feeding the object URLs to Anansi. The other component is Twine. Twine reads from an Anansi queue, processes and imports the RDF to the triple store.

[res-project]:         https://bbcarchdev.github.io/res/
[openrefine]:          http://openrefine.org/
